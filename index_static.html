<!DOCTYPE HTML>

<html lang="en" xmlns="">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yudi Ruan 阮宇迪</title>
    <meta name="author" content="YudiRuan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="src/images/pngs/cortex_logo.png" type="image/png">
    <link rel="stylesheet" href="css/desktop.css" media="screen and (min-width: 601px)">
    <link rel="stylesheet" href="css/mobile.css" media="screen and (max-width: 600px)">
    <style>
        /* Media query to hide the right column when screen width is less than 600px */
        @media (max-width: 600px) {
            .right-column {
                display: none;
            }

            .bottom-column {
                display: block;
            }
        }

        .inline-icon {
            vertical-align: middle; /* 使图标与文本对齐 */
            width: 24px; /* 设置图标的宽度 */
            height: 24px; /* 设置图标的高度 */
            margin-bottom: 4px;
        }

        @media (min-width: 600px) {
            .right-column {
                display: block;
            }

            .bottom-column {
                display: none;
            }
        }
    </style>
</head>

<body>
<div class="mobile-component">
    <ul>
        <li><a href="#Top"><strong>Yudi Ruan</strong></a></li>&nbsp;·&nbsp;
        <li><a href="#ResearchExperience">🔬</a></li>&nbsp;·&nbsp;
        <!--         <li><a href="#Education">📚</a></li>&nbsp;·&nbsp;-->
        <li><a href="#Research">💡</a></li>&nbsp;·&nbsp;
        <li><a href="#Projects">📂</a></li>&nbsp;·&nbsp;
        <li><a href="#Awards">🏆</a></li>
    </ul>
</div>

<div class="desktop-component">
    <ul>
        <li><a href="#Top" style="font-size: x-large;"><strong>Yudi Ruan</strong></a></li>&nbsp;·&nbsp;
        <li><a href="#ResearchExperience">Experience</a></li>&nbsp;·&nbsp;
        <li><a href="#Research">Research</a></li>&nbsp;·&nbsp;
        <li><a href="#Projects">Projects</a></li>&nbsp;·&nbsp;
        <li><a href="#Awards">Awards</a></li>
    </ul>

</div>

<script>
    // Get the height of the mobile and desktop navigation bars
    var navbarHeightMobile = document.querySelector('.mobile-component ul').offsetHeight;
    var navbarHeightDesktop = document.querySelector('.desktop-component ul').offsetHeight;

    // Add click event listeners to navigation links
    document.querySelectorAll('.mobile-component ul li a, .desktop-component ul li a, .a').forEach(function (anchor) {
        anchor.addEventListener('click', function (event) {
            event.preventDefault(); // Prevent the default navigation behavior

            var targetId = this.getAttribute('href'); // Get the target section's ID from the link's href
            var targetElement = document.querySelector(targetId); // Find the target element using its ID
            var targetOffsetTop = targetElement.offsetTop; // Get the target's top offset relative to the document

            // Calculate scroll position considering the navigation bar height to avoid overlap
            var navbarHeight = (window.innerWidth < 768) ? navbarHeightMobile : navbarHeightDesktop;
            window.scrollTo({
                top: targetOffsetTop - navbarHeight,
                behavior: 'smooth' // Smooth scroll to the target position
            });
        });
    });
</script>
<section id="Top"></section>
<table style="max-width:900px;margin:auto;padding-top: 30px;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <br>
            <em style="color: rgb(160, 160, 160);font-size: smaller;">You're accessing a static version for low-bandwidth. For a better
                experience, you can visit <a href="index_dynamic.html"><u>Link</u></a></em>
            <hr>
            <div class="bio">
                <div class="face-name">
                    <img src="src/images/pngs/profile.jpg" alt="profile photo" class="profile-img">
                    <div class="name-info">
                        <table style="width: 100%;">
                            <tr>
                                <td style="vertical-align: auto; width: auto; padding-right: 20px;">
                                    <p>
                                        阮宇迪 <br>
                                        Yudi Ruan<br>
                                    </p>
                                </td>
                                <td class="right-column" style="vertical-align: auto; width: auto; padding-left: 20px;">
                                    <p>
                                        <em style="color: rgb(160, 160, 160);font-size: 12px; padding-bottom: 0px">
                                            2023 National Undergraduate Scholarship &#127894<br>
                                            2023 Model Student<br>
                                            2023 Advanced Individual in Technology<br>
                                            2023 Outstanding Student
                                        </em>
                                    </p>
                                </td>
                            </tr>
                        </table>
                    </div>
                </div>
                <br>
                <div class="bottom-column">
                    <hr>
                    <p>
                        <em style="color: rgb(160, 160, 160);font-size: 12px; padding-bottom: 0px">
                            2023 National Undergraduate Scholarship &#127894 <br>
                            2023 Model Student<br>
                            2023 Advanced Individual in Technology<br>
                            2023 Outstanding Student
                        </em>
                    </p>
                </div>
                <hr>
                <p>
                    Hello, I am Ruan Yudi. I am currently an undergraduate student majoring in Artificial Intelligence
                    at the School of Information Science and Engineering, Chongqing Jiaotong University. My current job
                    focuses on Visual Language Model. At the same time, I am conducting market research and am
                    passionate about building an artificial intelligence company (CORTEX) that empowers traditional
                    industries.
                </p>
                <p>
                    <strong style="color: rgb(255, 67, 183);">I’m currently seeking a PhD or Master position for Fall
                        2026 admission.🌟</strong>
                </p>
                <div class="links">
                    <a href="">&#128279 CV</a> &nbsp;·&nbsp;
                    <!-- <a href="data/resume-zh.pdf">CV-zh</a> &nbsp;·&nbsp; -->
                    <a href="https://blog.csdn.net/m0_46197553?spm=1000.2115.3001.5343">
                        <img src="https://th.bing.com/th?id=ODLS.75fbaa62-ad18-4e8b-9ca8-67706ec82c93&w=32&h=32&qlt=90&pcl=fffffa&o=6&pid=1.2"
                             alt="GitHub Logo" class="inline-icon" style="width: 18px; height: 18px; ">
                        CSDN</a> &nbsp;·&nbsp;
                    <a href="https://github.com/ruanyudi">
                        <img src="/src/images/pngs/github-logo.png"
                             alt="GitHub Logo" class="inline-icon">
                        Github
                    </a> &nbsp;·&nbsp;
                    <a href="https://scholar.google.com/citations?user=_NxfkuYAAAAJ&hl=zh-CN">
                        <img src="/src/images/pngs/google-scholar-ico.png"
                             alt="Google scholar Logo" class="inline-icon" style="width: 18px; height: 18px; ">
                        Google Scholar</a> &nbsp;·&nbsp;
                    <a href="mailto:yudi.ruan@mails.cqjtu.edu.cn">&#128231 Email</a>
                </div>
            </div>

            <hr>
            <section id="Awards"></section>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2><strong>Awards 🏆</strong></h2>

                        <h3><strong>1) International</strong></strong></h3>
                        <p>
                            <strong>Honorable Mention</strong> &#128208
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                Mathematical Contest in Modeling / Interdisciplinary Contest in Modeling (MCM/ICM)
                            </em>

                        </p>

                        <h3><strong>2) National</strong></strong></h3>
                        <p>
                            <strong>1st. Prize &#129351</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                RAICOM Robotics Developer Competition (RAICOM) CAIR Engineering National Finals (2024)
                            </em>
                            <br>
                            <strong>2nd. Prize &#129352</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                The 26th China Robotics and Artificial Intelligence Competition National Finals (2024)
                            </em>
                            <br>
                            <strong>2nd. Prize &#129352</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                National College Business Elite Challenge National Finals (2024)
                            </em>
                            <br>
                            <strong>3rd. Prize &#129353</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                The 8th 'Hangda Data Cup' National College Intelligent Transportation Innovation and
                                Entrepreneurship Competition National Finals (2024)
                            </em>

                        </p>

                        <h3><strong>3) Provincial</strong></strong></h3>
                        <p>
                            <strong>1st. Prize &#129351</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                National College Student Mathematical Modeling Competition (2023)
                            </em>
                            <br>
                            <strong>2nd. Prize &#129352</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                The 18th National College Student Intelligent Vehicle Competition, iFlytek Track (2023)
                            </em>
                            <br>
                            <strong>2nd. Prize &#129352</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                The 18th National College Student Intelligent Vehicle Competition, Baidu Track (2023)
                            </em>
                            <br>
                            <strong>2nd. Prize &#129352</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                The 16th China College Students Computer Design Competition (2023)
                            </em>
                            <br>
                            <strong>3rd. Prize &#129353</strong>
                            <em style="color: rgb(160, 160, 160);font-size: smaller;">
                                China Software Cup University Student Software Design Competition (2023)
                            </em>

                        </p>

                    </td>
                </tr>
                </tbody>
            </table>

            <!--          <hr>-->
            <!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
            <!--            <tbody>-->
            <!--              <tr>-->
            <!--                <td style="padding:20px;width:100%;vertical-align:middle">-->
            <!--                  <h2><strong>News📰</strong></h2>-->
            <!--                </td>-->
            <!--              </tr>-->
            <!--            </tbody>-->
            <!--          </table>-->
            <!--  -->
            <!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
            <!--            <tbody>-->
            <!--              <tr>-->
            <!--                <td style="padding:20px;width:85%;vertical-align:middle">-->
            <!--                  <p style="color: rgb(59, 59, 59);font-size: larger;">-->
            <!--                    <strong>[2024/6/26]</strong> - PhD offer from CUHK!-->
            <!--                  </p>-->
            <!--                  <p style="color: rgb(59, 59, 59);font-size: larger;">-->
            <!--                    <strong>[2024/6/14]</strong> - Mask4Align now released!-->
            <!--                  </p>-->
            <!--                  <p style="color: rgb(59, 59, 59);font-size: larger;">-->
            <!--                    <strong>[2024/2/27]</strong> - My first paper accepted by CVPR 2024! See you in Seattle! -->
            <!--                  </p>-->
            <!--                </td>-->
            <!--              </tr>-->
            <!--            </tbody>-->
            <!--          </table>-->

            <hr>
            <section id="ResearchExperience"></section>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2><strong>Experience 🔬</strong></h2>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100px;vertical-align:middle">
                        <img src="src/images/pngs/cqjtu_logo.webp" alt="Your Image Alt Text"
                             style="width: 100%; height: auto;">
                    </td>

                    <td style="padding:20px;width:auto;vertical-align:middle">
                        <p><strong style="font-size: larger;">Chongqing Jiaotong University</strong>
                            <br>
                            <em>Undergraduate Student</em>
                            <br>
                            <em>Rank 1/67 GPA:4.08/5.00</em>
                            <br>

                            <br>
                            Few Shot Object Detection, Image Restoration
                            <br>
                            advised by Prof. Weikai Li
                        </p>

                        <p>
                            China. 2022/7 - present
                        </p>

                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100px;vertical-align:middle">
                        <img src="src/images/pngs/uvclinic.png" alt="Your Image Alt Text"
                             style="width: 100%; height: auto;">
                    </td>

                    <td style="padding:20px;width:auto;vertical-align:middle">
                        <p><strong style="font-size: larger;">Shanghai Panoramic Medical Imaging Technology Co.,
                            Ltd</strong>
                            <br>
                            <em>Research Intern</em>
                            <br>
                            <br>
                            Application of diagnosis of brain disorders
                            <br>
                        </p>
                        <p>
                            Shanghai, China. 2023/7 - 2024/9
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100px;vertical-align:middle">
                        <img src="src/images/pngs/cortex_logo.png" alt="Your Image Alt Text"
                             style="width: 100%; height: auto;">
                    </td>

                    <td style="padding:20px;width:auto;vertical-align:middle">
                        <p><strong style="font-size: larger;">CORTEX Technology Co., Ltd</strong>
                            <br>
                            <em>Co-Founder</em>
                            <br>
                            <br>
                            Embodied AI, RAG AI Agent
                            <br>
                        </p>
                        <p>
                            Taizhou, China. 2024/8 - present
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


            <hr>
            <section id="Research"></section>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2><strong>Research 💡</strong></h2>

                        <br>
                        <h2 style="font-size:large;"><strong>Interests:</strong></h2>
                        <br>
                        <div class="interest">
                            · <em><strong>Multi-Modality Representation Learning</strong></em> &#127775
                        </div>
                        <br>
                        <div class="interest">
                            · <em><strong>Domain Adaptation</strong></em> &#128257
                        </div>
                        <br>
                        <div class="interest">
                            · <em><strong>Few Shot Object Detection</strong></em> &#127919
                        </div>


                    </td>
                </tr>
                </tbody>
            </table>

            <section id="paper"></section>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/ECAFormer.png' alt="ECAFormer">
                </div>
                <div class="text">
                    <span class="papertitle">ECAFormer: Low-light Image Enhancement using Cross Attention</span><br>

                    <p>
                        <u><strong>Yudi Ruan</strong></u>,
                        Hao Ma,
                        Weikai Li,
                        Xiao Wang
                    </p>

                    <p>
                        Low-light image enhancement (LLIE) is critical in computer vision. Existing LLIE methods often
                        fail to discover the underlying relationships between different sub-components, causing the loss
                        of complementary information between multiple modules and network layers, ultimately resulting
                        in the loss of image details. To beat this shortage, we design a hierarchical mutual Enhancement
                        via a Cross Attention transformer (ECAFormer), which introduces an architecture that enables
                        concurrent propagation and interaction of multiple features. The model preserves detailed
                        information by introducing a Dual Multi-head self-attention (DMSA), which leverages visual and
                        semantic features across different scales, allowing them to guide and complement each other.
                        Besides, a Cross-Scale DMSA block is introduced to capture the residual connection, integrating
                        cross-layer information to further enhance image detail. Experimental results show that
                        ECAFormer reaches competitive performance across multiple benchmarks, yielding nearly a 3%
                        improvement in PSNR over the suboptimal method, demonstrating the effectiveness of information
                        interaction in LLIE.
                    </p>

                    <p>
                        <strong class="buttom"><a href="https://arxiv.org/abs/2406.13281v2">[Paper]</a></strong>
                        <!-- <strong class="buttom"><a href="https://github.com/HaoquanZhang/mask4align">[Code]</a></strong> -->
                    </p>
                    <strong>Keywords: Image Restoration, Cross Attention, Transformer, Feature fusion.</strong><br><br>
                    <div class="insub">
                        <strong>Under Review</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/MFCP.png' alt="MFCP">
                </div>
                <div class="text">
                    <span class="papertitle">MFCP: Identification of Major Depressive Disorder Using Multiple Functional Connection Pattern</span><br>

                    <p>
                        <u><strong>Yudi Ruan</u>*</strong>,
                        Ling Zhang*,
                        Liling Peng*,
                        Weikai Li,
                        Xin Gao
                        <br>
                        <em style="color: rgb(160, 160, 160);font-size: smaller;">* Co-first authorship</em>
                    </p>

                    <p>
                        Major depressive disorder (MDD) poses a significant challenge to global mental health,
                        necessitating the development of sophisticated diagnostic tools for its early and accurate
                        detection. Currently, rs-fMRI has attracted considerable attention in evaluating MDD through
                        functional brain networks (FBNs). The advent of graph convolutional networks (GCNs) has
                        revolutionized the analysis of FBNs by capturing the complex interregional connection patterns
                        that underlie neurological disorders, including MDD. However, existing GCN-based methodologies
                        have predominantly concentrated on the examination of FBNs through a single topological
                        structure, thereby neglecting the rich, multifaceted information encoded within various
                        connection patterns. This limitation hinders the full realization of GCNs' potential in MDD
                        diagnosis. To address this, we propose the Multiple Functional Connection Pattern Graph
                        Convolutional Network (MFCP), an innovative framework that integrates three distinct connection
                        patterns—Sparse Representation (SR), Partial Correlation (PC), and Granger Causality Mapping
                        (GCM)—to harness the synergistic insights they provide. Our preliminary investigation integrates
                        multiple graph convolutional modules to amalgamate diverse connection information, thereby
                        enriching the MDD diagnostic features extracted from FBNs. To evaluate the proposed MFCP, we
                        conduct experiment on the REST-MDD dataset with 533 subjects. The experimental results indicate
                        that our MFCP attained an accuracy rate of 87.74% and an AUC score of 0.9326, confirming the
                        effectiveness of our MFCP.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: Resting-state fMRI; Connection patterns; Functional brain network; Graph
                        convolutional network; Major depressive disorders.</strong><br><br>
                    <div class="insub">
                        <strong>Under Review</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/Manuscript.png' alt="Manuscript">
                </div>
                <div class="text">
                    <span class="papertitle">Contrastive Clustering-driven Topological Organization Unveils Characterization of Hybrid-rice with In-Field for Variety Purification</span><br>

                    <p>
                        Huaiqu Feng,
                        Te Xi,
                        <u><strong>Yudi Ruan</strong></u>,
                        Dunhong Yang,
                        Yulei Pan,
                        Rongkai Shi,
                        Bo Chen,
                        Yongwei Wang,
                        Jun Wang
                        <br>
                    </p>

                    <p>
                        Hybrid-rice seed production technology is a significant contributor to the high yield. The way
                        to improve seed production's yield and quality is purification. With the functional requirements
                        of the intelligent Undesired-rice removal process, the environmental elements of field operation
                        and the morphological parameters of Desired-rice and Undesired-rice are collected and measured.
                        The hybrid rice phenotypic of the multiple growth stages was collected to explore topological
                        organization unveiling the hybrid rice in-field for variety purification. The phenological
                        growth stages include the elongation, booting, heading, and filling stages. Expert knowledge on
                        identifying abnormal mature varieties within hybrid rice was incorporated to ensure accuracy in
                        variety purification. Then, the data was collected using the self-supervised learning
                        visualization method for deep cluster analysis. The proposed method, the t-SimCLR algorithm,
                        uses Contrastive Learning and neighbor embeddings to visualize high- dimensional data. The
                        parametric mapping is trained from the high-dimensional pixel space into two dimensions and
                        achieves classification accuracy by the t-SimCLR algorithm. The semantic relationships of
                        Undesired-rice and Desired-rice data are faithfully captured. Finally, the preliminary results
                        from the expert's prior knowledge are compared with the deep cluster analysis results to explore
                        the topological organization of the hybrid rice in-field. Last, the integration of expert prior
                        knowledge in the detection of panicles was showcased. The study culminates in demonstrating the
                        integration of expert knowledge in the detection of panicles, which is crucial for accurately
                        identifying and detecting abnormal mature varieties. This study contributes to the advancement
                        of hybrid rice purification techniques and sets the stage for developing more sophisticated
                        intelligent systems for agricultural applications.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: Hybrid-rice, Phenotypic, Visualization, Data integration, Contrastive
                        learning.</strong><br><br>
                    <div class="insub">
                        <strong>Under Review</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/tide.png' alt="tide">
                </div>
                <div class="text">
                    <span class="papertitle">TIDE: Test-Time Few Shot Object Detection</span><br>

                    <p>
                        Weikai Li,
                        Hongfeng Wei,
                        Yanlai Wu,
                        Jie Yang,
                        <u><strong>Yudi Ruan</strong></u>,
                        Yuan Li,
                        Ying Tang
                        <br>
                    </p>

                    <p>
                        Few-shot object detection (FSOD) aims to extract semantic knowledge from limited object
                        instances of novel categories within a target domain. Recent advances in FSOD focus on
                        fine-tuning the base model based on a few objects via meta-learning or data augmentation.
                        Despite their success, the majority of them are grounded with parametric readjustment to
                        generalize on novel objects, which face considerable challenges in Industry 5.0, such as 1) a
                        certain amount of fine-tuning time is required and 2) the parameters of the constructed model
                        being unavailable due to the privilege protection, making the fine-tuning fail. Such constraints
                        naturally limit its application in scenarios with real-time configuration requirements or within
                        black-box settings. To tackle the challenges mentioned above, we formalize a novel FSOD task,
                        referred to as test-time few-shot detection (TIDE), where the model is un-tuned in the
                        configuration procedure. To that end, we introduce an asymmetric architecture for learning a
                        support-instance-guided dynamic category classifier. Further, a cross-attention module and a
                        multiscale resizer are provided to enhance the model performance. Experimental results on
                        multiple FSOD platforms reveal that the proposed TIDE significantly outperforms existing
                        contemporary methods.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: Cross Attention, Few-shot Detection, Test Time.</strong><br><br>
                    <div class="pub">
                        <strong>IEEE Transactions on Systems, Man, and Cybernetics Systems</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/toward.png' alt="toward">
                </div>
                <div class="text">
                    <span class="papertitle">Towards an accurate autism spectrum disorder diagnosis: multiple connectome views from fMRI data</span><br>

                    <p>
                        Jie Yang*,
                        Xiaowen Xu*,
                        Mingxiang Sun*,
                        <u><strong>Yudi Ruan</strong></u>,
                        Chenhao Sun,
                        Weikai Li,
                        Xin Gao
                        <br>
                        <em style="color: rgb(160, 160, 160);font-size: smaller;">* Co-first authorship</em>
                    </p>

                    <p>
                        Functional connectome has revealed remarkable potential in the diagnosis of neurological
                        disorders, e.g. autism spectrum disorder. However, existing studies have primarily focused on a
                        single connectivity pattern, such as full correlation, partial correlation, or causality. Such
                        an approach fails in discovering the potential complementary topology information of FCNs at
                        different connection patterns, resulting in lower diagnostic performance. Consequently, toward
                        an accurate autism spectrum disorder diagnosis, a straight-forward ambition is to combine the
                        multiple connectivity patterns for the diagnosis of neurological disorders. To this end, we
                        conduct functional magnetic resonance imaging data to construct multiple brain networks with
                        different connectivity patterns and employ kernel combination techniques to fuse information
                        from different brain connectivity patterns for autism diagnosis. To verify the effectiveness of
                        our approach, we assess the performance of the proposed method on the Autism Brain Imaging Data
                        Exchange dataset for diagnosing autism spectrum disorder. The experimental findings demonstrate
                        that our method achieves precise autism spectrum disorder diagnosis with exceptional accuracy
                        (91.30%), sensitivity (91.48%), and specificity (91.11%).
                    </p>

                    <p>
                        <!--                <strong class="buttom"><a href="https://arxiv.org/abs/2406.13281v2">[Paper]</a></strong>-->
                        <!-- <strong class="buttom"><a href="https://github.com/HaoquanZhang/mask4align">[Code]</a></strong> -->
                    </p>
                    <strong>Keywords: autism spectrum disorder; multi kernel learning; partial correlation; Pearson’s
                        correlation; Granger causality.</strong><br><br>
                    <div class="pub">
                        <strong>Cerebral Cortex</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/MPGCN.png' alt="MPGCN">
                </div>
                <div class="text">
                    <span class="papertitle">Multipattern graph convolutional network-based autism spectrum disorder identification</span><br>

                    <p>
                        Wenhao Zhou*,
                        Mingxiang Sun*,
                        Xiaowen Xu*,
                        <u><strong>Yudi Ruan</strong></u>,
                        Chenhao Sun,
                        Weikai Li,
                        Xin Gao
                        <br>
                        <em style="color: rgb(160, 160, 160);font-size: smaller;">* Co-first authorship</em>
                    </p>

                    <p>
                        The early diagnosis of autism spectrum disorder (ASD) has been extensively facilitated through
                        the utilization of resting-state fMRI (rs-fMRI). With rs-fMRI, the functional brain network
                        (FBN) has gained much attention in diagnosing ASD. As a promising strategy, graph convolutional
                        networks (GCN) provide an attractive approach to simultaneously extract FBN features and
                        facilitate ASD identification, thus replacing the manual feature extraction from FBN. Previous
                        GCN studies primarily emphasized the exploration of topological simultaneously connection
                        weights of the estimated FBNs while only focusing on the single connection pattern. However,
                        this approach fails to exploit the potential complementary information offered by different
                        connection patterns of FBNs, thereby inherently limiting the performance. To enhance the
                        diagnostic performance, we propose a multipattern graph convolution network (MPGCN) that
                        integrates multiple connection patterns to improve the accuracy of ASD diagnosis. As an initial
                        endeavor, we endeavored to integrate information from multiple connection patterns by
                        incorporating multiple graph convolution modules. The effectiveness of the MPGCN approach is
                        evaluated by analyzing rs-fMRI scans from a cohort of 92 subjects sourced from the publicly
                        accessible Autism Brain Imaging Data Exchange database. Notably, the experiment demonstrates
                        that our model achieves an accuracy of 91.1% and an area under ROC curve score of 0.9742.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: resting-state fMRI; autism spectrum disorder; multipattern; brain connectivity
                        networks; graph convolution network.</strong><br><br>
                    <div class="pub">
                        <strong>Cerebral Cortex</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/FewSegment.png' alt="FewSegment">
                </div>
                <div class="text">
                    <span class="papertitle">Few-shot segmentation with duplex network and attention augmented module</span><br>

                    <p>
                        Sifu Zeng,
                        Jie Yang,
                        Wang Luo,
                        <u><strong>Yudi Ruan</strong></u>
                        <br>
                    </p>

                    <p>
                        Establishing the relationship between a limited number of samples and segmented objects in
                        diverse scenarios is the primary challenge in few-shot segmentation. However, many previous
                        works overlooked the crucial support-query set interaction and the deeper information that needs
                        to be explored. This oversight can lead to model failure when confronted with complex scenarios,
                        such as ambiguous boundaries. To solve this problem, a duplex network that utilizes the
                        suppression and focus concept is proposed to effectively suppress the background and focus on
                        the foreground. Our network includes dynamic convolution to enhance the support-query
                        interaction and a prototype match structure to fully extract information from support and query.
                        The proposed model is called dynamic prototype mixture convolutional networks (DPMC). To
                        minimize the impact of redundant information, we have incorporated a hybrid attentional module
                        called double-layer attention augmented convolutional module (DAAConv) into DPMC. This module
                        enables the network to concentrate more on foreground information. Our experiments on PASCAL-5i
                        and COCO-20i datasets suggested that DPMC and DAAConv outperform traditional prototype-based
                        methods by up to 5–8% on average.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: few-shot segmentation, semantic segmentation, mixture models, duplex mode,
                        attention module</strong><br><br>
                    <div class="pub">
                        <strong>Frontiers in Neurorobotics</strong>
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/AlteredASD.png' alt="AlteredASD">
                </div>
                <div class="text">
                    <span class="papertitle">Altered intra- and inter-network connectivity in autism spectrum disorder</span><br>

                    <p>
                        Rui Zhou*,
                        Chenhao Sun*,
                        Mingxiang Sun*,
                        <u><strong>Yudi Ruan</strong></u>,
                        Weikai Li,
                        Xin Gao
                        <br>
                        <em style="color: rgb(160, 160, 160);font-size: smaller;">* Co-first authorship</em>
                    </p>

                    <p>
                        A neurodevelopmental illness termed as the autism spectrum disorder (ASD) is described by social
                        interaction impairments. Previous studies employing resting-state functional imaging (rs-fMRI)
                        identified both hyperconnectivity and hypoconnectivity patterns in ASD people. However, specific
                        patterns of connectivity within and between networks linked to ASD remain largely unexplored.
                        Methods: We utilized a meticulously selected subset of high-quality data, comprising 45
                        individuals diagnosed with ASD and 47 HCs, obtained from the ABIDE dataset. The pre-processed
                        rs-fMRI time series signals were partitioned into ninety regions of interest. We focused on
                        eight intrinsic connectivity networks and further performed intra- and inter-network analysis.
                        Finally, support vector machine was used to discriminate ASD from HC. Results: Through different
                        sparsities, ASD exhibited significantly decreased intra-network connectivity within default mode
                        network and dorsal attention network, increased connectivity between limbic network and
                        subcortical network, and decreased connectivity between default mode network and limbic network.
                        Using the classifier trained on altered intra- and inter-network connectivity, multivariate
                        pattern analyses classified the ASD from HC with 71.74% accuracy, 70.21% specificity and 75.56%
                        sensitivity in 10% sparsity of functional connectivity. Conclusions: ASD showed characteristic
                        reorganization of the brain networks and this provided new insight into the underlying process
                        of the functional connectome dysfunction in ASD.
                    </p>

                    <p>
                        <!--                <strong class="buttom">[Paper]</strong>-->
                        <!--                <strong class="buttom">[Code]</strong>-->
                    </p>
                    <strong>Keywords: autism spectrum disorder, inter-network connectivity, intra-network connectivity,
                        support vector machine, functional brain network</strong><br><br>
                    <div class="pub">
                        <strong>Aging</strong>
                    </div>
                </div>
            </div>

            <hr>
            <section id="Projects"></section>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2><strong>Projects 📂</strong></h2>
                    </td>
                </tr>
                </tbody>
            </table>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/robotics.png' alt="robotics">
                </div>
                <div class="text">
                    <span class="papertitle">RX7: hexapod spider robot</span><br>
                    <!--              <strong><a href="NBMEC_2023/B030079.pdf">[Entry (Chinese)]</a></strong>-->
                    <p>
                        It integrates NVIDIA Jetson series development kits and Intel Realsense cameras, equipped with
                        16 high-precision servos. The robot's body is designed and cut from carbon fiber plates.
                        Currently, it is capable of autonomous mapping and navigation,
                        as well as target tracking, making it suitable for exploring complex terrains and performing
                        various tasks. The future work involves integrating LLMs to create an embodied AI.
                    </p>
                    <!--              <em>Entry</em>, 2023, National Biomedical Engineering Innovation Design Competition for College Students-->
                    <!--              <p><strong>Second Prize. (6%)</strong></p>-->
                    <strong>Keywords: SLAM, Mechanical Design </strong><br><br>
                    <div class="pub">
                        Under Development
                    </div>
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/MedSegVisualizer.png' alt="MedSegVisualizer">
                </div>
                <div class="text">
                    <span class="papertitle">Huanshi: 3D Medical Imaging Data Analysis Platform</span><br>
                    <!--              <span class="papertitle">Design of Auxiliary Diagnosis Algorithm for Schizophrenia Based on Feature Fusion of EEG and ECG</span><br>-->
                    <!--              <strong><a href="NBMEC_2023/B030079.pdf">[Entry (Chinese)]</a></strong>-->
                    <p>
                        This project is an AI-based 3D medical imaging data analysis system that utilizes
                        state-of-the-art machine vision models. It intelligently analyzes multimodal medical imaging
                        data, including CT and MRI scans, to automatically segment and visualize relevant areas, such as
                        15 abdominal
                        organs and tumors. The project employs a Client/Server (C/S) architecture, with
                        visualization handled on the client side via ITK, and data augmentation and model inference
                        managed on the server side. The server side utilizes the industry-leading
                        Kubernetes (K8S) distributed architecture for cloud computing, deploying inference engines in
                        containers to achieve automatic scaling and fault tolerance, resulting in high performance and
                        high availability.
                    </p>
                    <strong>Keywords: K8S, Deep-learning, Medical Science </strong><br><br>
                    <div class="pub">
                        Chongqing Provincial Innovation Project
                    </div>
                    <!--              <p><strong>Second Prize. (6%)</strong></p>-->
                </div>
            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/autoware.png' alt="autoware">
                </div>
                <div class="text">
                    <div class="papertitle">Train Multi-Source Fusion Perception System</div>
                    <br>
                    <p>
                        It utilizes NVIDIA Jetson series development kits and Intel Realsense series
                        cameras. It aims to measure the real-time velocity and location of urban trains through visual
                        (monocular/stereo) and accelerometer data, constructing a visual inertial odometry system to
                        replace traditional wheel encoders.
                    </p>
                    <strong>Keywords: SLAM, Visual Odometry</strong><br><br>
                    <div class="pub">
                        Chinese National Innovation Project
                    </div>
                </div>

            </div>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/LLIEVisualizer.png' alt="LLIEVisualizer">
                </div>
                <div class="text">
                    <div class="papertitle">Image Enhancement and Analysis System</div>
                    <br>
                    <p>
                        An integrated platform that integrates image enhancement models and high-level vision
                        models, aimed at assisting with dataset annotation and enhancement. It uses ONNX for model
                        deployment.
                    </p>
                    <strong>Keywords: QT, Model Deployment</strong><br><br>
                    <div class="pub">
                        Under Development
                    </div>
                    <!--              <p><strong>Second Prize. (6%)</strong></p>-->
                </div>
            </div>


            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/RAGAgent.png' alt="RAGAgent">
                </div>
                <div class="text">
                    <div class="papertitle"> CORTEX Chat Agent</div>
                    <p>
                        Built a customizable AI agent based on RAG technology, which helps to mitigate LLM
                        hallucinations and accelerate the deployment of LLMs. It is inspired by NVIDIA's online course
                        <a href="https://www.nvidia.cn/training/instructor-led-workshops/building-rag-agents-with-llms/">"Building
                            RAG Agent with LLMs"</a>.
                    </p>
                    <strong>Keywords: SLMs, LLMs, RAG</strong><br><br>
                    <div class="pub">
                        Under Development
                    </div>
                </div>
            </div>

            <h3 style="font-size: 1.5em; /* Adjust the font size */
            font-weight: bold; /* Make the text bold */
            color: #2a9d8f; /* Change the text color */
            text-align: center; /* Center align the text */
            margin: 20px 0; /* Add some margin above and below the heading */">
                Empowering Traditional Industries with Knowledge: </h3>

            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/Tocore.png' alt="Tcore">
                </div>
                <div class="text">
                    <div class="papertitle">Web Design for Tocore</div>
                    <br>
                    <p></p>
                    <strong>Keywords: HTML, CSS, JavaScript, Nginx</strong><br><br>
                    <!--              <p><strong>Second Prize. (6%)</strong></p>-->
                </div>
            </div>
            <div class="paper-container">
                <div class="image">
                    <img src='src/images/pngs/FingerprintExtractor.png' alt="FingerprintExtractor">
                </div>
                <div class="text">
                    <div class="papertitle">FingerPrint Extractor designed for Safe Security</div>
                    <br>
                    <p></p>
                    <strong>Keywords: Tracking, FingerPrint, Thinning Methods</strong><br><br>
                    <a href="https://github.com/ruanyudi/FingerPrint">
                        [
                        <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png"
                             alt="GitHub Logo" class="inline-icon">
                        Code]
                    </a>
                    <!--              <p><strong>Second Prize. (6%)</strong></p>-->
                </div>
            </div>

            <!-- <div class="paper-container">
              <div class="image">
                <img src='gifs/eeg-monitor.png' alt="EEG Monitor">
              </div>
              <div class="text">
                <span class="papertitle">Limbs Motor Function Monitoring System Based on EEG and EMG Detection and Analysis</span><br>
                <strong><a href="LimbMonitor/Poster.pdf">[Poster]</a></strong> ·
                <strong><a href="LimbMonitor/bme-2022-intro.pptx">[Poster Source File (.pptx)]</a></strong>
                <br><br>
                <em>Course design</em>, 2021, Exploration and Design of Biomedical Engineering
                <br>
                <p>
                  Built an automatic classification system to assess the subject’s weight-bearing status based on EEG and EMG. This design is an exploration of the ability of EEG and EMG to assess the motor status of stroke patients.
                </p>
                <p><strong>Final Score: 92, 4.0/4.0.</strong></p>
              </div>
            </div> -->


            <!-- <hr>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h3><strong>Misc 🤔</strong></h3>

                  <p>· I love anime. Recently I've been watching <a href="https://makeine-anime.com/"><em>Too Many Losing Heroines!</em></a>. </p>
                  <p>· I occasionally make tracks, mainly around focused on EDM.</p>
                  <p>· I occasionally make tracks, mainly around focused on EDM.</p>

                </td>
              </tr>
            </tbody></table> -->

            <!-- <hr>
            <section id="Friends"></section>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                  <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2><strong>Friends 🤜🤛</strong></h2>
                    <br>
                  <div class="image-text-container">
                    <div class="image">
                        <img src="gifs/scut_logo.png">
                    </div>

                    <div class="text">
                    <table>
                      <tr>
                          <td><strong><a href="https://tobyleelsz.github.io/">Shangzhe Li</a></strong></td>
                          <td>Research Intern @UCSD</td>
                          <td><em>Reinforcement Learning · physics enthusiast · pilot</em></td>
                      </tr>
                      <tr>
                          <td><strong><a href="https://xinjie-shen.com/">Xinjie Shen</a></strong></td>
                          <td>Research Intern @Dartmouth</td>
                          <td><em>Interaction · Graph · Quantitative Finance</em></td>
                      </tr>
                      <tr>
                          <td><strong><a href="https://brandon-liu-jx.github.io/">Jinxiu Liu</a></strong></td>
                          <td>Research Intern @Stanford</td>
                          <td><em>4D Dynamic Generation · MLLM</em></td>
                      </tr>
                      <tr>
                          <td><strong><a href="https://troychowzyb.github.io/">Yubin Zhou</a></strong></td>
                          <td>Research Intern @BrainCo</td>
                          <td><em>Brain-Computer Interface · Cognitive Neuroscience</em></td>
                      </tr>
                    </table>
                  </div>

                  </div>

                  <div class="image-text-container">
                    <div class="image">
                        <img src="gifs/NYU.jpg">
                    </div>
                    <div class="text">
                      <table>
                        <tr>
                          <td><strong>Junru Liao</strong></td>
                          <td>Undergraduate @NYU</td>
                          <td><em>Biomechanics · Cellular Mechanics Response</em></td>
                        </tr>
                      </table>
                    </div>
                  </div>

                  <div class="image-text-container">
                    <div class="image">
                        <img src="gifs/Sun_Yat-sen_University_Logo.png">
                    </div>
                    <div class="text">
                      <h2 style="padding-bottom:10px;">Sun Yat-sen University</h2>

                      <strong>HONG Xuan</strong>
                       - <em>High-Energy Phenomenology · Particle Physics · Cosmology<br>

                      <strong>FAN Wei</strong>
                       - <em>Piezoelectricity · Semiconductor · DeviceFabrication<br>
                    </div>
                  </div>

                  </td>
              </tr>
              </tbody>
            </table> -->

            <hr>
            <!--          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding: 20px;">-->
            <!--            <tbody>-->
            <!--              <tr>-->
            <!--                <td style="padding:0px">-->
            <!--                  <p style="text-align:center;">-->
            <!--                    Thanks to <a href="https://github.com/jonbarron/jonbarron_website">John Barron</a> for this homepage template.-->
            <!--                    <br>-->
            <!--                    Feel free to take the <a href="https://github.com/HaoquanZhang/HaoquanZhang.github.io">resources</a> of this page.-->
            <!--                  </p>-->
            <!--                  <p style="text-align:center;color: rgb(143, 143, 143);">-->
            <!--                    © 2024 Haoquan Zhang-->
            <!--                  </p>-->
            <!--                  <div class="logo-container">-->
            <!--                    <img src="gifs/scut_logo.png" alt="SCUT Logo">-->
            <!--                    <img src="gifs/gzic.jpg" alt="GZIC">-->
            <!--                    <img src="gifs/smu.svg" alt="SMU Logo">-->
            <!--                    <img src="gifs/CUHK.png" alt="CUHK Logo">-->
            <!--                </div>-->
            <!--                </td>-->
            <!--              </tr>-->
            <!--            </tbody>-->
            <!--          </table>-->

        </td>
    </tr>
</table>
</body>
</html>

